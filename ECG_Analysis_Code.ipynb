{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa4fntzmyMEzPteyFf5o7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anirudh2465/Semester1/blob/ECG-Analysis/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scipy wfdb vmdpy"
      ],
      "metadata": {
        "id": "MSZ24gX30zsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wfdb\n",
        "from scipy import stats\n",
        "from vmdpy import VMD\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "63wnwAAP0vVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the VMD parameters\n",
        "alpha = 2000  # Bandwidth constraint\n",
        "tau = 0.  # Noise-tolerance (no strict fidelity enforcement)\n",
        "K = 3  # 3 modes\n",
        "DC = 0  # No DC part imposed\n",
        "init = 1  # Initialize omegas uniformly\n",
        "tol = 1e-7  # Tolerance\n",
        "\n",
        "L_data=[109,101,233,104,207,220]\n",
        "\n",
        "# Initialize a list to store the results\n",
        "u_chunks = []\n",
        "ecg_data=[]\n",
        "for j in L_data:\n",
        "  # Load the MIT-BIH Arrhythmia dataset\n",
        "  record = wfdb.rdrecord(f'/content/{j}')\n",
        "\n",
        "  # Get the ECG data\n",
        "  ecg_data1 = record.p_signal.flatten()\n",
        "\n",
        "  ecg_data.append(ecg_data1)\n",
        "\n",
        "  # Define the size of each chunk\n",
        "  chunk_size = 10000  # Adjust this value based on your resources\n",
        "\n",
        "  # Calculate the number of chunks\n",
        "  num_chunks = len(ecg_data1) // chunk_size\n",
        "\n",
        "\n",
        "\n",
        "  # Loop over each chunk\n",
        "  for i in range(num_chunks):\n",
        "      # Get the start and end indices of the chunk\n",
        "      start = i * chunk_size\n",
        "      end = start + chunk_size\n",
        "\n",
        "      # Get the chunk of data\n",
        "      chunk = ecg_data1[start:end]\n",
        "\n",
        "      # Apply VMD to the chunk\n",
        "      u_chunk, u_hat_chunk, omega_chunk = VMD(chunk, alpha, tau, K, DC, init, tol)\n",
        "\n",
        "      # Append the result to the list\n",
        "      u_chunks.append(u_chunk)\n",
        "\n",
        "\n",
        "      print(\"Done VDM\",j, \"chunk\",i+1)\n",
        "\n",
        "# Concatenate the chunks along the second axis\n",
        "u = np.concatenate(u_chunks, axis=1)\n",
        "ecg_data=np.concatenate(ecg_data)\n",
        "\n",
        "def normalize(data):\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    return (data - mean) / std\n",
        "\n",
        "def signal_to_noise_ratio(signal, noise):\n",
        "    signal_power = np.mean(np.square(signal))\n",
        "    noise_power = np.mean(np.square(noise))\n",
        "    return 10 * np.log10(signal_power / noise_power)\n",
        "\n",
        "u=normalize(u)\n",
        "\n",
        "# Plot the original ECG data and the IMFs\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(len(u) + 1, 1, 1)\n",
        "plt.plot(ecg_data)\n",
        "plt.title('Original ECG data')\n",
        "\n",
        "for i, IMF in enumerate(u, start=2):\n",
        "    plt.subplot(len(u) + 1, 1, i)\n",
        "    plt.plot(IMF)\n",
        "    plt.title(f'IMF {i - 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "snrval = signal_to_noise_ratio(u, ecg_data - u)\n",
        "print(f'Signal-to-Noise Ratio after VMD: {snrval}')\n",
        "\n",
        "# Initialize an empty list to store the features\n",
        "features = []\n",
        "\n",
        "# Initialize an empty list to store the labels\n",
        "labels = []\n",
        "\n",
        "for j in L_data:\n",
        "  # Read the annotation\n",
        "  annotation = wfdb.rdann(f'/content/{j}', 'atr')\n",
        "\n",
        "  # Loop over each beat location\n",
        "  for i in range(1, len(annotation.sample)):\n",
        "    if annotation.symbol[i] in ['N','L','R','j','e','A','S','J','a','!','V','E','[',']','F','f','/','Q']:\n",
        "        # Get the start and end of the beat\n",
        "        start = annotation.sample[i-1]\n",
        "        end = annotation.sample[i]\n",
        "\n",
        "        # Initialize a list to store the features for this beat\n",
        "        beat_features = []\n",
        "\n",
        "        # Loop over each IMF\n",
        "        for imf in u:\n",
        "            # Segment the IMF at the beat location\n",
        "            segment = imf[start:end]\n",
        "\n",
        "            # Calculate the mean, standard deviation, skewness, and kurtosis of the segment\n",
        "            mean = np.mean(segment)\n",
        "            std = np.std(segment)\n",
        "            skewness = skew(segment)\n",
        "            kurt = kurtosis(segment)\n",
        "\n",
        "            # Append the features to the beat_features list\n",
        "            beat_features.extend([mean, std, skewness, kurt])\n",
        "\n",
        "        # Append the beat_features to the features list\n",
        "        features.append(beat_features)\n",
        "\n",
        "        # Append the label to the labels list\n",
        "        labels.append(annotation.symbol[i])\n",
        "  print(\"Feature extraction done for data set : \", j)\n",
        "# Convert the lists to NumPy arrays\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "s9yR_Wx90nZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **VMD Parameters**: You're defining parameters for Variational Mode Decomposition (VMD), a method for decomposing a signal into multiple components, or modes. The parameters include the bandwidth constraint (`alpha`), noise tolerance (`tau`), number of modes (`K`), DC part (`DC`), initialization method (`init`), and tolerance (`tol`).\n",
        "\n",
        "2. **Data Loading**: You're loading ECG data from the MIT-BIH Arrhythmia dataset. The dataset is divided into chunks of size 10000, and VMD is applied to each chunk. The results are stored in `u_chunks`.\n",
        "\n",
        "3. **Signal Normalization**: After applying VMD to all chunks and concatenating the results, you normalize the signal. Normalization is a common preprocessing step in machine learning that makes the data have zero mean and unit variance.\n",
        "\n",
        "4. **Signal-to-Noise Ratio (SNR)**: You're calculating the SNR of the normalized signal. SNR is a measure of signal strength relative to background noise. The higher the ratio, the less obtrusive the background noise is.\n",
        "\n",
        "5. **Feature Extraction**: For each beat in the ECG data, you're extracting features from the output of VMD. These features include the mean, standard deviation, skewness, and kurtosis of each Intrinsic Mode Function (IMF) segment corresponding to the beat. The features are stored in `features`, and the corresponding labels are stored in `labels`.\n",
        "\n",
        "6. **Data Conversion**: Finally, you're converting the lists of features and labels to NumPy arrays for further processing. This is a common practice as many machine learning algorithms require input data to be in the form of NumPy arrays.\n",
        "\n",
        "This is a comprehensive approach to signal processing and feature extraction for ECG data."
      ],
      "metadata": {
        "id": "akQULZFp0noQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcHvBf4pzCna"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "val = scaler.fit_transform(features)\n",
        "\n",
        "smote = SMOTE(k_neighbors=1)\n",
        "\n",
        "# Fit the SMOTE transformer to the data\n",
        "smote.fit(val, labels)\n",
        "\n",
        "# Resample the data\n",
        "val_resampled, labels_resampled = smote.fit_resample(val, labels)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(val_resampled, labels_resampled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = HistGradientBoostingClassifier()\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"Model 1 trained\")\n",
        "\n",
        "pred1 = model1.predict(X_test)\n",
        "accuracy = np.mean(pred1 == y_test)\n",
        "print(f'Accuracy of model 1: {accuracy}')\n",
        "\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, pred1, average='weighted')\n",
        "recall = recall_score(y_test, pred1, average='weighted')\n",
        "f1 = f1_score(y_test, pred1, average='weighted')\n",
        "\n",
        "# Print the scores\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Generate confusion matrix\n",
        "matrix = confusion_matrix(y_test, pred1)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=L2,\n",
        "            yticklabels=L)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1nwrqAAX79C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = svm.SVC()\n",
        "model2.fit(X_train,y_train)\n",
        "print(\"Model 2 trained\")\n",
        "\n",
        "pred2 = model2.predict(X_test)\n",
        "accuracy = np.mean(pred2 == y_test)\n",
        "print(f'Accuracy of model 2: {accuracy}')\n",
        "\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, pred2, average='weighted')\n",
        "recall = recall_score(y_test, pred2, average='weighted')\n",
        "f1 = f1_score(y_test, pred2, average='weighted')\n",
        "\n",
        "# Print the scores\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Generate confusion matrix\n",
        "matrix = confusion_matrix(y_test, pred2)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=L2,\n",
        "            yticklabels=L)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G7KIBxPpX-uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = RandomForestClassifier()\n",
        "model3.fit(X_train,y_train)\n",
        "print(\"Model 3 trained\")\n",
        "\n",
        "pred3 = model3.predict(X_test)\n",
        "accuracy = np.mean(pred3 == y_test)\n",
        "print(f'Accuracy of model 3: {accuracy}')\n",
        "\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, pred3, average='weighted')\n",
        "recall = recall_score(y_test, pred3, average='weighted')\n",
        "f1 = f1_score(y_test, pred3, average='weighted')\n",
        "\n",
        "# Print the scores\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Generate confusion matrix\n",
        "matrix = confusion_matrix(y_test, pred3)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=L2,\n",
        "            yticklabels=L)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xhWXSIWTYJB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = DecisionTreeClassifier()\n",
        "model4.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "pred4 = model4.predict(X_test)\n",
        "accuracy = np.mean(pred4 == y_test)\n",
        "print(f'Accuracy of Decision Tree model: {accuracy}')\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, pred4, average='weighted')\n",
        "recall = recall_score(y_test, pred4, average='weighted')\n",
        "f1 = f1_score(y_test, pred4, average='weighted')\n",
        "\n",
        "# Print the scores\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Generate confusion matrix\n",
        "matrix4 = confusion_matrix(y_test, pred4)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(matrix4, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=L2,\n",
        "            yticklabels=L)\n",
        "\n",
        "plt.title('Confusion Matrix for Decision Tree')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "blGXL4y-YRT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `scaler = StandardScaler()`: This line initializes a `StandardScaler` object from the `sklearn.preprocessing` module. The `StandardScaler` standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "2. `val = scaler.fit_transform(features)`: The `fit_transform` method computes the mean and standard deviation on `features` for later scaling (fit part) and then scales `features` (transform part). The transformed data is stored in `val`.\n",
        "\n",
        "3. `smote = SMOTE(k_neighbors=1)`: This line initializes a `SMOTE` object from the `imblearn.over_sampling` module with `k_neighbors` set to 1. SMOTE stands for Synthetic Minority Over-sampling Technique. It is a technique used to handle class imbalance. It works by creating synthetic samples from the minor class instead of creating copies.\n",
        "\n",
        "4. `smote.fit(val, labels)`: This line fits the SMOTE object to the data. It doesn't change the data but allows the object to gather necessary information about the data.\n",
        "\n",
        "5. `val_resampled, labels_resampled = smote.fit_resample(val, labels)`: The `fit_resample` method resamples the dataset. It applies the synthetic over-sampling process to the dataset and returns the resampled feature set `val_resampled` and label set `labels_resampled`.\n",
        "\n",
        "6. `X_train, X_test, y_train, y_test = train_test_split(val_resampled, labels_resampled, test_size=0.2, random_state=42)`: This line splits the resampled dataset into training and test sets. 80% of the data will be used for training and 20% for testing.\n",
        "\n",
        "7. `model1 = HistGradientBoostingClassifier()`: This line initializes a `HistGradientBoostingClassifier` object from the `sklearn.ensemble` module. This is a histogram-based Gradient Boosting Classification Tree.\n",
        "\n",
        "8. `model1.fit(X_train, y_train)`: This line fits the model to the training data.\n",
        "\n",
        "9. `pred1 = model1.predict(X_test)`: This line uses the fitted model to make predictions on the test data.\n",
        "\n",
        "10. `accuracy = np.mean(pred1 == y_test)`: This line calculates the accuracy of the model by comparing the predicted labels to the true labels.\n",
        "\n",
        "11. `precision = precision_score(y_test, pred1, average='weighted')`: This line calculates the weighted precision of the model. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
        "\n",
        "12. `recall = recall_score(y_test, pred1, average='weighted')`: This line calculates the weighted recall of the model. Recall (Sensitivity) - the ratio of correctly predicted positive observations to all observations in actual class.\n",
        "\n",
        "13. `f1 = f1_score(y_test, pred1, average='weighted')`: This line calculates the weighted F1 score of the model. The F1 score is the harmonic mean of precision and recall.\n",
        "\n",
        "14. `matrix = confusion_matrix(y_test, pred1)`: This line generates a confusion matrix, which is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\n",
        "\n",
        "15. The last few lines of your code are used to create a heatmap of the confusion matrix using the seaborn library. This provides a visual representation of the performance of your model. The x and y labels of the heatmap represent the classes of your data. The color and the number in each cell represent the number of instances of actual class (row) predicted as the predicted class (column).\n"
      ],
      "metadata": {
        "id": "JjcHYdyZ0cL4"
      }
    }
  ]
}
